---
title: "9 - Metropolis Hastings Proof"
date: 2019-05-24
categories: Bayesian
mathjax: true
---



In this post, we will see how Metropolis-Hasting algorithm converges to target distribution. Before we get into the proof, we need to know some of the notions.

<p align = 'center'>
    <img src = '../../assets/img/bayesian/9-periodic.png' style='width:70%'>
    <br/>
    <sub>Periodic state</sub>
</p>

- **Periodic**: If the state $$i$$ is visited after a number of steps that is an integer greater than 1, then the state is called periodic. For instance, if state 0 is visited in above plot, there should be two steps to return to state 0. So the state is periodic. If one can return to any state $$i$$ at anytime, it is called **Aperiodic**. 

<p align = 'center'>
    <img src = '../../assets/img/bayesian/9-reducible.png' style='width:70%'>
    <br/>
    <sub>Reducible state</sub>
</p>

- **Irreducible**: One can get to any other state z from any state x with probability greater than zero. The opposite is reducible. If you look at the above plot, when it gets to state 3 or 0, it is absorbed and cannot get out, which is **reducible** to the orange rectangle.
- **Ergodic**: One have both irreducible and aperiodic properties.
- **Transient**: Let's denote $$\tau_{ii}$$  is amount of time until the chain returns to the state $$i$$ given that is started at state $$i$$. If the chain has $$p(\tau_{ii} < \infin )  = 1$$, the probability of ever returning to state $$i$$ given started $$i$$, it is called **recurrent**. On the other hand if it is $$p(\tau_{ii}<\infin) < 1$$, it is called **transient**.



In order to prove Metropolis-Hastings converges to the target distribution, there are two steps. First, we have to show the Markov chain gets to a unique stationary distribution. Next, we need to prove the stationary distribution equals to the target distribution.

For the first part, the Markov chain has a unique stationary distribution if it is **irreducible**, **aperiodic** and **not transient**(recurrent). The latter two conditions hold for a random walk on any proper distribution, and irreducibility holds as long as we choose to use a distribution with positive probability of reaching from any state to any other state.

Now we have to show the target distribution is the stationary distribution of the Markov chain generated by Metropolis-Hastings.

Recall some of notations:

$$ \theta =$$ state

$$ J_t(\cdot \mid \cdot) = $$ Jumping or proposal distribution from one state to other

$$p(\cdot) =$$ Target distribution



Consider we start the Metropolis algorithm at time $$t-1$$ with a draw $$\theta^{t-1}$$ from the target distribution $$p(\theta \mid y) $$. Then unconditional probability of getting $$\theta_a$$ at time $$t-1$$ and $$\theta_b $$ at time $$t$$ is, assuming $$p(\theta_b \mid y) > p(\theta_a \mid y) $$:

$$ p(\theta^{t-1} = \theta_a, \theta^t = \theta_b) = p(\theta_a \mid y) J_t(\theta_b \mid \theta_a) min( \frac{p(\theta_b \mid y)}{p(\theta_a \mid y)}, 1)$$

$$ = p(\theta_a \mid y) J_t(\theta_b \mid \theta_a) $$

Similarly we can define the unconditional probability of getting $$\theta_b$$ at time $$t-1$$ and $$\theta_b$$ at time $$t$$ is:

$$ p(\theta^{t-1}=\theta_b, \theta^t = \theta_a) = p(\theta_b \mid y) J_t(\theta_a \mid \theta_b)  min( \frac{p(\theta_a \mid y)}{p(\theta_b \mid y)}, 1) $$

$$ = p(\theta_b \mid y) J_t(\theta_a \mid \theta_b) \frac{p(\theta_a \mid y)}{p(\theta_b \mid y)} $$

$$ = p(\theta_a \mid y) J_t(\theta_a \mid \theta_b)$$

Since the Metropolis algorithm uses symmetric transitional distribution, $$J_t(\theta_a \mid \theta_b) = J_t(\theta_b \mid \theta_a) $$. Therefore,



## Reference

- STAT 578: Advanced Bayesian Modeling by Professor Trevor Park
- Gelman, A., Stern, H. S., Carlin, J. B., Dunson, D. B., Vehtari, A., & Rubin, D. B. (2013). Bayesian data analysis. Chapman and Hall/CRC.



