---
title: "MA-POCA"
date: 2024-03-17
categories: rl
mathjax: true
tags: [RL, ml-agents, Unity]
---

# 배경
- 개요
    - PVP 게임에서 유저 대신 봇으로 대체하는 경우가 있음
        - 매칭할 유저가 적거나
        - 유저와 매칭할 실력이 안되는 초보자를 케어하기 위해
    - 이러한 경우 봇 플레이가 단조로워 아쉬웠음
        - 일반 유저와 비교하여 실력 차이가 크게 나고, 단일 난이도로 구성
        - 미션이 주어진 경우에는 협동해서 플레이하기가 어려움
            - 특정 지역으로 같이 이동하는데 전투 지역을 벗어나 있음
            - 전투 시 도움이 필요할 때 도와주지 않음
        - FPS 게임에서는 이따금씩 특정 구역에서 가만히 서성이고 있다던가
        - 은폐/엄폐 없이 전투를 진행
    - 위 같은 점들 때문에 매치에 봇이 포함되면 플레이 긴장도가 크게 낮아짐
- 목적
    - 기존에 어떻게 봇을 만들고 왜 만들기 어려운 지 파악
    - 다른 방식을 통해 위 봇 문제점을 해결할 수 있을지 테스트


## 기존 시스템 이해
기존 봇을 만들 때 자주 사용되는 Behavior Tree에 대한 이해

- Behavior Tree
    - 우선 순위와 트리 구조를 이용해 인공지능을 설계하는 방법
    - 모듈화가 자유로워 확장이 쉬움
    - Halo2에서 사용되고 [GDC 2005년 소개](https://www.gamedeveloper.com/programming/gdc-2005-proceeding-handling-complexity-in-the-i-halo-2-i-ai#close-modal)
- 구성 요소
    - Root, Composite, Action
    - 항상 왼쪽에 있는 노드에 우선 순위를 부여

<p align ='center'>
    <img src = "../../assets/img/rl/mapoca_fps_behavior_tree.PNG" style="width: 70%"> <br/>
    <sub>Behavior Tree 예시</sub>
</p>

- Composite 노드에서 다수의 행동을 컨트롤 함
  - Selector: 여러 행동 중 하나의 행동 선택
  - Sequence: 여러 행동을 순차적으로 수행
  - Parallel: 여러 행동을 함께 수행
- Composite 노드 내 부가 요소
  - Decorator: 노드가 실행되는 조건(조건문)(파란색)
  - Service: 노드가 활성화될 때 실행되는 부가 명령
  - Abort: Decorator 조건에 부합하면 노드 내 활동 모두 중단(빨간색)
- Black Board
  - Behavior Tree에서 행동을 결정할 때 필요한 모든 정보를 저장
- 그 외로도 State Machine, GOAP(Goal Oriented Action Planning) 등을 사용

## 강화 학습 문제로 해결?
- 주어진 환경(Environment)과 상호 작용하여 봇(Agent)이 최종 목표를 달성할 수 있도록 학습하는 문제
    -  상호 작용이란 환경에 대한 관찰 정보(State, Observation)와 이에 대한 봇의 행동(Action) 그리고 이에 따른 보상(Reward)
- 특정 상황에서 특정 행동을 하도록 설정하지 않음
- State, Action, Reward를 바탕으로 Agent가 어떻게 행동할 지에 대한 정책(Policy) 학습

### 강화 학습 구성 요소

<p align ='center'>
    <img src = "https://blog.kakaocdn.net/dn/LRYmt/btqMPhgOTcd/PWilCM3GmsRnDjOO9IdFN0/img.png" style="width: 70%"> <br/>
    <sub>강화 학습 구성 요소</sub>
</p>

- Agent
  - 봇
  - Action(Actuator)
  - Agent가 취할 수 있는 행동
- Environment
  - 환경
  - Observation
  - Agent가 인식할 수 있는 환경 정보
- step
  - agent가 받는 정보의 시간 단위를 step이라고 함(t로 표기)
  - 각 step 별 Environment에서 state(observation)을 받고 그에 따른 action을 취함
  - 하나의 게임을 Episode라 하고, Episode는 여러 step으로 이루어짐

모델을 사용한다면 이렇게 풀 수 있지 않을까? 

딥러닝 모델을 사용하여 목표 지표를 최대화하는 각 상황 → 행동 매핑 함수를 만들 수 있음

<p align ='center'>
    <img src = "../../assets/img/rl/rl_model.PNG" style="width: 70%"> <br/>
    <sub></sub>
</p>

## 강화 학습 모델 설명
강화 학습을 푸는 여러 방법이 있지만 그중에서 많이 사용되는 알고리즘을 간략하게 설명

가치 신경망 모델

- 현재 상황과 agent 행동에 대한 기대되는 보상을 예측
- 필요한 이유는 한 게임(Episode)에 대한 보상(reward)은 한번만 주어짐
  - 이기거나 지거나 등
- 어떤 상황에서 어떤 행동이 해당 보상에 기여했는지 알기 어려움
- 특정 시점(step)에서의 환경(State)에 대한 보상을 모델링


정책 신경망 모델

- 현재 상황에서 각 행동에 대한 확률값을 모델링
- 각 상황에 대한 위 가치 판단 모델을 사용하여 action을 평가하고, 가치 모델이 더 높은 보상을 받을 수 있도록 action을 학습

# 모델 테스트

# DodgeBall 프로젝트

# 내용 정리
- 목적
    - 유니티에서 만든 강화 학습 알고리즘 Multi-Agent POsthumous Credit Assignment(MA-POCA) 내용 정리
    - 논문 내용
MA-POCA
On the Use and Misuse of Absorbing States in Multi-agent Reinforcement

#### 확인 사항
- ml-agents 패키지에서는 value function과 q function 파라미터를 동일하게 사용
- 

Learning