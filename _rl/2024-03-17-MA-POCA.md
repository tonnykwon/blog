---
title: "MA-POCA"
date: 2024-03-17
categories: rl
mathjax: true
tags: [RL, ml-agents, Unity]
---

# 배경
- 개요
    - PVP 게임에서 유저 대신 봇으로 대체하는 경우가 있음
        - 매칭할 유저가 적거나
        - 유저와 매칭할 실력이 안되는 초보자를 케어하기 위해
    - 이러한 경우 봇 플레이가 단조로워 아쉬웠음
        - 일반 유저와 비교하여 실력 차이가 크게 나고, 단일 난이도로 구성
        - 미션이 주어진 경우에는 협동해서 플레이하기가 어려움
            - 특정 지역으로 같이 이동하는데 전투 지역을 벗어나 있음
            - 전투 시 도움이 필요할 때 도와주지 않음
        - FPS 게임에서는 이따금씩 특정 구역에서 가만히 서성이고 있다던가
        - 은폐/엄폐 없이 전투를 진행
    - 위 같은 점들 때문에 매치에 봇이 포함되면 플레이 긴장도가 크게 낮아짐
- 목적
    - 기존에 어떻게 봇을 만들고 왜 만들기 어려운 지 파악
    - 다른 방식을 통해 위 봇 문제점을 해결할 수 있을지 테스트


## 기존 시스템 이해
기존 봇을 만들 때 자주 사용되는 Behavior Tree에 대한 이해

- Behavior Tree
    - 우선 순위와 트리 구조를 이용해 인공지능을 설계하는 방법
    - 모듈화가 자유로워 확장이 쉬움
    - Halo2에서 사용되고 [GDC 2005년 소개](https://www.gamedeveloper.com/programming/gdc-2005-proceeding-handling-complexity-in-the-i-halo-2-i-ai#close-modal)
- 구성 요소
    - Root, Composite, Action
    - 항상 왼쪽에 있는 노드에 우선 순위를 부여

<p align ='center'>
    <img src = "../../assets/img/rl/mapoca_fps_behavior_tree.PNG" style="width: 70%"> <br/>
    <sub>Behavior Tree 예시</sub>
</p>

- Composite 노드에서 다수의 행동을 컨트롤 함
  - Selector: 여러 행동 중 하나의 행동 선택
  - Sequence: 여러 행동을 순차적으로 수행
  - Parallel: 여러 행동을 함께 수행
- Composite 노드 내 부가 요소
  - Decorator: 노드가 실행되는 조건(조건문)(파란색)
  - Service: 노드가 활성화될 때 실행되는 부가 명령
  - Abort: Decorator 조건에 부합하면 노드 내 활동 모두 중단(빨간색)
- Black Board
  - Behavior Tree에서 행동을 결정할 때 필요한 모든 정보를 저장
- 그 외로도 State Machine, GOAP(Goal Oriented Action Planning) 등을 사용

## 강화 학습 문제로 해결?
- 주어진 환경(Environment)과 상호 작용하여 봇(Agent)이 최종 목표를 달성할 수 있도록 학습하는 문제
    -  상호 작용이란 환경에 대한 관찰 정보(State, Observation)와 이에 대한 봇의 행동(Action) 그리고 이에 따른 보상(Reward)
- 특정 상황에서 특정 행동을 하도록 설정하지 않음
- State, Action, Reward를 바탕으로 Agent가 어떻게 행동할 지에 대한 정책(Policy) 학습

### 강화 학습 구성 요소

<p align ='center'>
    <img src = "https://blog.kakaocdn.net/dn/LRYmt/btqMPhgOTcd/PWilCM3GmsRnDjOO9IdFN0/img.png" style="width: 70%"> <br/>
    <sub>강화 학습 구성 요소</sub>
</p>

- Agent
  - 봇
  - Action(Actuator)
  - Agent가 취할 수 있는 행동
- Environment
  - 환경
  - Observation
  - Agent가 인식할 수 있는 환경 정보
- step
  - agent가 받는 정보의 시간 단위를 step이라고 함(t로 표기)
  - 각 step 별 Environment에서 state(observation)을 받고 그에 따른 action을 취함
  - 하나의 게임을 Episode라 하고, Episode는 여러 step으로 이루어짐

모델을 사용한다면 이렇게 풀 수 있지 않을까? 

딥러닝 모델을 사용하여 목표 지표를 최대화하는 각 상황 → 행동 매핑 함수를 만들 수 있음

<p align ='center'>
    <img src = "../../assets/img/rl/rl_model.PNG" style="width: 70%"> <br/>
    <sub></sub>
</p>

## 강화 학습 모델 설명
강화 학습을 푸는 여러 방법이 있지만 그중에서 많이 사용되는 알고리즘을 간략하게 설명

가치 신경망 모델

- 현재 상황과 agent 행동에 대한 기대되는 보상을 예측
- 필요한 이유는 한 게임(Episode)에 대한 보상(reward)은 한번만 주어짐
  - 이기거나 지거나 등
- 어떤 상황에서 어떤 행동이 해당 보상에 기여했는지 알기 어려움
- 특정 시점(step)에서의 환경(State)에 대한 보상을 모델링


정책 신경망 모델

- 현재 상황에서 각 행동에 대한 확률값을 모델링
- 각 상황에 대한 위 가치 판단 모델을 사용하여 action을 평가하고, 가치 모델이 더 높은 보상을 받을 수 있도록 action을 학습

# 모델 테스트

## 강화학습 테스트
- 간단한 문제를 설정하여 강화 학습으로 풀 수 있는 지 실험
- 기존에 관심 있었던 FPS 게임으로 먼저 테스트
- FPS에서 기본적인 기능을 강화 학습 모델로 풀 수 있는 지 개념 증명
  - 기본적인 기능
    - 대상 인식하기
      - 현재 위치에서 인식 가능한 범위 내에 대상이 있을 때, 대상이 있는 지 확인 가능
    - 대상 찾아가기
      - 전체 이동 가능한 공간에서 대상이 어디 있는 지 탐험

#### 테스트 환경 선택
- 유니티 mlagents VS 언리얼엔진 learning agent
  - 언리얼엔진이 behavior tree 개발 툴을 기본적으로 제공하고 그래픽이 더 사실적이나, 강화 학습 모듈인 learning agent 관련 정보가 적음
  - 그에 비해 Unity는 mlagents 관련 정보가 인터넷에 많아 Unity를 사용하기로 선택 


### 대상 인식하기
- 현재 위치에서 대상(적)이 어느 위치에 있는 지 확인하고 인식하는 기능을 테스트
- 어떤 방식으로 Agent가 대상을 인식할 수 있는 지 파악하고 얼마나 잘 파악하는 지 확인
  - 어떤 방식 → 어떤 데이터를 주었을 때
  - 인식 → 대상을 맞추는 행위로 정의

#### Ray Cast Sensor(Observation)
- 이미지에 비해 처리할 데이터가 적기 때문에 ray cast를 Agent의 observation으로 sensor 자주 사용
- 각 ray마다 tag가 있는 object를 맞았냐 안 맞았냐 boolean값 반환

<p align ='center'>
    <img src = "../../assets/img/rl/raycast.PNG" style="width: 70%"> <br/>
    <sub>Raycast</sub>
</p>
- point A(x, y, z)에서 point B로 선을 그리는 것
- 총 쏘기(투사체 없음)에 주로 사용
  - 미리 인식이 필요한 타겟 object에 tag를 붙임
  - ray cast가 해당 tag에 맞으면 관련 정보를 받음
  - 관련 정보로 ray cast가 맞았는 지, 맞았을 때 거리 등등을 얻을 수 있음

#### 환경 세팅 정리
| 종류          | 변수                   | 설명                                                |
|-------------|----------------------|---------------------------------------------------|
| Observation | Target Ray Cast 6개   | - 각 Ray Cast 별 타겟이 맞았는 지 여부<br>- 값: 0, 1<br>- 거리 제한: 50 |
|             | Agent 현재 y축 rotation | - agent의 현재 y축 roattion 값<br>- 값 범위: -90 ~ 90         |
| Action      | y축 roatation move    | - agent y축으로 회전<br>- 값 범위: -90 ~ 90                   |
|             | shoot                | - 총 발사                                              |
| Reward      | target shoot         | - 타겟 상자를 맞추었을 때 +1                                  |
|             | count down reward    | - 매 step마다 -1/max_step<br>- 빠르게 게임을 끝나도록 유도           |

빠른 속도 영상 예시
<p align ='center'>
    <img src = "../../assets/img/rl/240101model_successgunammo-ezgif.com-video-to-gif-converter.gif" style="width: 70%"> <br/>
    <sub>대상 인식</sub>
</p>
한쪽 방향으로 돌면서 상자를 잘 맞춤

<br>
일반 속도 영상
<p align ='center'>
    <img src = "../../assets/img/rl/240108model-ezgif.com-video-to-gif-converter.gif" style="width: 70%"> <br/>
    <sub>대상 인식</sub>
</p>
→  Ray Cast 사이에 있는 물체를 가끔 못 맞추지만 의도대로 동작

### 대상 찾아가기
#### 맵에서 대상 찾기
- FPS에서 전투에 진입 전까지 맵을 돌아다니며 적의 위치를 파악
- 전체 맵에서 대상의 위치 정보 없이 맵을 탐험하여 대상을 찾을 수 있는 지 테스트
  - 대상을 찾음 → Agent의 정해 놓은 시야 범위(사각뿔대)에 들어오는 지로 파악
  - 현재 Agent 위치 정보 추가

#### 환경 세팅 정리
| 종류          | 변수                      | 설명                                                 |
|-------------|-------------------------|----------------------------------------------------|
| Observation | Wall Ray Cast(6개)       | 각 Ray Cast 별 벽에 맞았는 지 여부<br>값: 0, 1<br>거리 제한: 50   |
|             | Target Ray Cast(6개)     | 각 Ray Cast 별 대상이 맞았는 지 여부<br>값: 0, 1<br>거리 제한: 50  |
|             | Agent y축 rotation       | agent의 현재 y축 roattion 값<br>값 범위: -90 ~ 90          |
|             | Agent x,z축 position(2개) | 높이(y)값은 고정되어 있으므로 x축, z축 값만 반환<br>값 범위: -inf ~ inf |
| Action      | x축 move                 | agent 앞, 뒤로 움직임<br>값 범위: -1 ~ 1                    |
|             | y축 roatation move       | agent y축으로 회전<br>값 범위: -90 ~ 90                    |
| Reward      | target found            | 대상을 찾았을 때(사각뿔대 범위에 들어왔을 때)를 맞추었을 때 +1              |
|             | count down reward       | 매 step마다 -1/max_step<br>빠르게 게임을 끝나도록 유도            |

영상 예시
<p align ='center'>
    <img src = "../../assets/img/rl/raycayst_model_fail.gif" style="width: 70%"> <br/>
    <sub>찾아가기 실패</sub>
</p>

- 대상을 찾는 경우보다 못 찾는 경우가 더 잦음
- 원인을 Ray Cast의 한계로 추측
  - 0, 1로만 물체/벽을 인식하기 때문에 ray cast가 맞은 지점까지의 거리를 알기 어려움
  - 그렇기 때문에 agent 위치와 목표물의 위치 파악이 어려움
  - 맵에서 어디를 탐색하였고 하지 않았는지 확인이 어려움

# DodgeBall 프로젝트

# 내용 정리
- 목적
    - 유니티에서 만든 강화 학습 알고리즘 Multi-Agent POsthumous Credit Assignment(MA-POCA) 내용 정리
    - 논문 내용
MA-POCA
On the Use and Misuse of Absorbing States in Multi-agent Reinforcement

#### 확인 사항
- ml-agents 패키지에서는 value function과 q function 파라미터를 동일하게 사용
- 

Learning